{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65f4de1",
   "metadata": {},
   "source": [
    "Dinh Hoang Viet Phuong - 301123263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7bbd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for building a TensorFlow-based deep learning model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress warnings to keep the output clean\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ccd937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to the Python script you want to load\n",
    "file_path = 'X:\\\\DinhHoangVietPhuong_COMP258Lab3_Ex01.py'\n",
    "\n",
    "# Open the specified file in read mode and store its contents in the 'data' variable\n",
    "with open(file_path, 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df75d77e-3f4f-474d-a8ae-c17f29b41024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Create a Tokenizer object with char_level=True to tokenize characters in the text\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "# Calculate vocabulary size by adding 1 to the length of the word index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert the input text data to sequences of tokens using the tokenizer\n",
    "sequences = tokenizer.texts_to_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a3627f-d1a8-4f3d-80e7-ffa87b3012b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: #!/usr/bin/env python\n",
      "# coding: utf-8\n",
      "\n",
      "# # Dinh Hoang Viett Phuong - 301123263\n",
      "\n",
      "# In[1]:\n",
      "\n",
      "\n",
      "# Importing necessary libraries and suppressing warnings for clean code execution\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import tensorflow as tf\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from tensorflow.keras.utils import to_categorical\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "\n",
      "\n",
      "# In[2]:\n",
      "\n",
      "\n",
      "# Read the Dataset\n",
      "df = pd.read_csv('D:\\\\Download\\\\student_data.csv', skiprows=24, header=1)\n",
      "print(df.head())\n",
      "\n",
      "\n",
      "# In[3]:\n",
      "\n",
      "\n",
      "# Read the CSV file, skip the first 24 rows, and set no header initially\n",
      "df = pd.read_csv('D:\\\\Download\\\\student_data.csv', skiprows=24, header=None)\n",
      "df.replace('?', np.nan, inplace=True)\n",
      "\n",
      "# Assign headers to the DataFrame\n",
      "headers = [\n",
      "    \"First Term Gpa\", \"Second Term Gpa\", \"First Language\", \"Funding\",\n",
      "    \"School\", \"FastTrack\", \"Coop\", \"Residency\", \"Ge\n",
      "\n",
      "Data length: 5517\n",
      "\n",
      "Sample tokenized sequence: [[27], [52], [49], [15], [5], [8], [49], [33], [6], [7]]\n",
      "\n",
      "Word Index (size): 56\n"
     ]
    }
   ],
   "source": [
    "# Check data and tokenization\n",
    "print(\"Sample data:\", data[:1000])  # Print the first 1000 characters of the data\n",
    "print(\"\\nData length:\", len(data))  # Print the length of the data\n",
    "print(\"\\nSample tokenized sequence:\", sequences[:10])  # Print the first 10 tokenized sequences\n",
    "print(\"\\nWord Index (size):\", len(tokenizer.word_index))  # Print the size of the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec5b844-1852-4242-a31d-a63c6c70c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input sequences: 5516\n"
     ]
    }
   ],
   "source": [
    "# Preparing input data\n",
    "input_sequences = []\n",
    "\n",
    "# Create n-gram sequences for input data\n",
    "for i in range(1, len(sequences)):\n",
    "    n_gram_sequence = sequences[:i+1]\n",
    "    input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Check if input_sequences is populated\n",
    "print(\"Number of input sequences:\", len(input_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "211420f2-8bee-4182-8d47-ba0cdbc525fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create predictors and label for training\n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=vocab_size)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=max_sequence_len - 1),\n",
    "    LSTM(100),  # Reduced complexity\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8fabbf-0ea6-47b0-8277-c8d23f821a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2/44 [>.............................] - ETA: 12:02:59 - loss: 4.0412"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "model.fit(predictors, label, epochs=10, batch_size=128, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Generate Fake Python Code\n",
    "# Define a function to generate fake Python code\n",
    "def generate_code(seed_text, model, tokenizer, max_sequence_len, num_chars=100):\n",
    "    for _ in range(num_chars):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        next_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
    "        next_char = tokenizer.index_word[next_index]\n",
    "        seed_text += next_char\n",
    "    return seed_text\n",
    "\n",
    "# Generate a new code snippet\n",
    "new_code = generate_code(\"# New Python\\n\", max_sequence_len)\n",
    "print(new_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8eab7-514e-46f7-a235-a83faaa66d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
